{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/balandongiv/advance_ODE-Progress-Bar-and-Interrupt-using-App-Designer/blob/master/openai_whisper_python_tutorial.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2f5f6009-6ff0-4bb8-8610-2e25495268b6"
      },
      "source": [
        "# OpenAI Whisper Python Tutorial"
      ],
      "id": "2f5f6009-6ff0-4bb8-8610-2e25495268b6"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "31fb9e7b"
      },
      "source": [
        "## 1. Getting Started with OpenAI Whisper"
      ],
      "id": "31fb9e7b"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9d283009"
      },
      "source": [
        "### a. Install OpenAI Whisper Python Library"
      ],
      "id": "9d283009"
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "1bf78c41-c455-4427-af2c-6ed7a8e3004b",
        "outputId": "843695c4-0d0a-45c8-d995-8435f8072ffe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting openai-whisper\n",
            "  Downloading openai-whisper-20231117.tar.gz (798 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m798.6/798.6 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: triton<3,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from openai-whisper) (2.1.0)\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.10/dist-packages (from openai-whisper) (0.58.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from openai-whisper) (1.23.5)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from openai-whisper) (2.1.0+cu121)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai-whisper) (4.66.1)\n",
            "Requirement already satisfied: more-itertools in /usr/local/lib/python3.10/dist-packages (from openai-whisper) (10.1.0)\n",
            "Collecting tiktoken (from openai-whisper)\n",
            "  Downloading tiktoken-0.5.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m34.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from triton<3,>=2.0.0->openai-whisper) (3.13.1)\n",
            "Requirement already satisfied: llvmlite<0.42,>=0.41.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba->openai-whisper) (0.41.1)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken->openai-whisper) (2023.6.3)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken->openai-whisper) (2.31.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper) (3.1.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper) (2023.6.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper) (2023.11.17)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->openai-whisper) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->openai-whisper) (1.3.0)\n",
            "Building wheels for collected packages: openai-whisper\n",
            "  Building wheel for openai-whisper (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for openai-whisper: filename=openai_whisper-20231117-py3-none-any.whl size=801356 sha256=1593b3f80d81d790fcc0bec82439ea5a72df0851ea5b082e93a6f5bef1d8d041\n",
            "  Stored in directory: /root/.cache/pip/wheels/d0/85/e1/9361b4cbea7dd4b7f6702fa4c3afc94877952eeb2b62f45f56\n",
            "Successfully built openai-whisper\n",
            "Installing collected packages: tiktoken, openai-whisper\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "llmx 0.0.15a0 requires cohere, which is not installed.\n",
            "llmx 0.0.15a0 requires openai, which is not installed.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed openai-whisper-20231117 tiktoken-0.5.2\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.6/57.6 kB\u001b[0m \u001b[31m913.1 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pydub\n",
            "  Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
            "Installing collected packages: pydub\n",
            "Successfully installed pydub-0.25.1\n",
            "Collecting python-docx\n",
            "  Downloading python_docx-1.1.0-py3-none-any.whl (239 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m239.6/239.6 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: lxml>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from python-docx) (4.9.4)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from python-docx) (4.5.0)\n",
            "Installing collected packages: python-docx\n",
            "Successfully installed python-docx-1.1.0\n"
          ]
        }
      ],
      "source": [
        "!pip install -U openai-whisper\n",
        "!pip install pytube -q\n",
        "!pip install pydub\n",
        "!pip install python-docx\n",
        "\n"
      ],
      "id": "1bf78c41-c455-4427-af2c-6ed7a8e3004b"
    },
    {
      "cell_type": "code",
      "source": [
        "from pydub import AudioSegment\n",
        "import os\n",
        "from pytube import YouTube\n",
        "import re\n",
        "import pandas as pd\n",
        "from pytube import YouTube\n",
        "from moviepy.editor import VideoFileClip\n",
        "from PIL import Image\n",
        "import os\n",
        "from docx import Document\n",
        "from docx.shared import Inches\n",
        "from PIL import Image\n",
        "import whisper\n",
        "model = whisper.load_model(\"small\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1wsR3e6WU3G0",
        "outputId": "3af212a3-c5b0-4bcc-8276-61e14daf5194"
      },
      "id": "1wsR3e6WU3G0",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|███████████████████████████████████████| 461M/461M [00:05<00:00, 82.1MiB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "6gBreIQjly2r"
      },
      "outputs": [],
      "source": [
        "def clean_and_format_string(input_string):\n",
        "    # Remove special characters and spaces, and replace them with underscores\n",
        "    cleaned_string = re.sub(r'[^a-zA-Z0-9]+', '_', input_string)\n",
        "\n",
        "    # Remove leading and trailing underscores\n",
        "    cleaned_string = cleaned_string.strip('_')\n",
        "\n",
        "    # Convert to lowercase if needed\n",
        "    formatted_string = cleaned_string.lower()\n",
        "\n",
        "    return formatted_string"
      ],
      "id": "6gBreIQjly2r"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bb1cb6e9"
      },
      "source": [
        "### b. Load the OpenAI Whisper Model"
      ],
      "id": "bb1cb6e9"
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "c989af14-2a18-4390-91be-d8e63d623fe2"
      },
      "outputs": [],
      "source": [],
      "id": "c989af14-2a18-4390-91be-d8e63d623fe2"
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "4OTlcOhB2jlH"
      },
      "outputs": [],
      "source": [
        "# from pydub import AudioSegment\n",
        "# import os\n",
        "\n",
        "def segment_audio(audio_path, segment_duration_seconds=20):  # segment_duration in seconds\n",
        "    # Convert segment duration to milliseconds\n",
        "    segment_duration_milliseconds = segment_duration_seconds * 1000\n",
        "\n",
        "    # Load audio file\n",
        "    audio = AudioSegment.from_file(audio_path)\n",
        "\n",
        "    # Create a directory to store segments\n",
        "    base_path, ext = os.path.splitext(audio_path)\n",
        "    segment_dir = base_path + \"_segments\"\n",
        "    if not os.path.exists(segment_dir):\n",
        "        os.makedirs(segment_dir)\n",
        "\n",
        "    # Segment audio and export\n",
        "    segment_paths = []\n",
        "    for i in range(0, len(audio), segment_duration_milliseconds):\n",
        "        segment = audio[i:i + segment_duration_milliseconds]\n",
        "        segment_path = os.path.join(segment_dir, f\"segment_{i // segment_duration_milliseconds}{ext}\")\n",
        "        segment.export(segment_path, format=ext.replace('.', ''))\n",
        "        segment_paths.append(segment_path)\n",
        "\n",
        "    return segment_paths\n"
      ],
      "id": "4OTlcOhB2jlH"
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "y5P7BbfUm4LJ"
      },
      "outputs": [],
      "source": [
        "def download_audio_from_youtube(youtube_video_url):\n",
        "    # folder_storage = 'Concept Selection process'\n",
        "    youtube_video_content = YouTube(youtube_video_url)\n",
        "    video_name = youtube_video_content.streams[0].default_filename\n",
        "\n",
        "    folder_storage = clean_and_format_string(video_name)\n",
        "\n",
        "\n",
        "    audio_streams = youtube_video_content.streams.filter(only_audio=True)\n",
        "    # for stream in audio_streams:\n",
        "    #     print(stream)\n",
        "\n",
        "    audio_stream = audio_streams[1]\n",
        "    dpath = audio_stream.download(folder_storage)\n",
        "    # print(audio_stream)\n",
        "    return dpath"
      ],
      "id": "y5P7BbfUm4LJ"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Get png image segment"
      ],
      "metadata": {
        "id": "skAF5EnC96pO"
      },
      "id": "skAF5EnC96pO"
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "def download_youtube_video(url, path='video.mp4'):\n",
        "    \"\"\"Download a YouTube video using pytube and return the title.\"\"\"\n",
        "    print(\"Downloading YouTube video...\")\n",
        "    yt = YouTube(url)\n",
        "    video_title = yt.title\n",
        "    output_fname = create_valid_folder_name(video_title)\n",
        "    path=f'{output_fname[:10]}.mp4'\n",
        "    stream = yt.streams.filter(progressive=True, file_extension='mp4').first()\n",
        "    stream.download(filename=path)\n",
        "    print(f\"Video {video_title} has been downloadeded successfully.\")\n",
        "    return path, video_title\n",
        "\n",
        "def create_valid_folder_name(title):\n",
        "    \"\"\"Create a valid folder name from the video title.\"\"\"\n",
        "    valid_name = ''.join(char for char in title if char.isalnum() or char in [' ', '_']).rstrip()\n",
        "    return valid_name.replace(' ', '_')\n",
        "\n",
        "def format_frame_name(seconds):\n",
        "    \"\"\"Format the frame name using minute and second format.\"\"\"\n",
        "    minutes = seconds // 60\n",
        "    seconds = seconds % 60\n",
        "    # return f\"frame_{minutes:02d}_{seconds:02d}.png\"\n",
        "    return f\"{minutes:02d}_{seconds:02d}.png\"\n",
        "\n",
        "def capture_frames(video_path, interval=5, output_folder='frames'):\n",
        "    \"\"\"Capture frames from the video at given interval.\"\"\"\n",
        "    print(f\"Capturing frames from the video into folder '{output_folder}'...\")\n",
        "    if not os.path.exists(output_folder):\n",
        "        os.makedirs(output_folder)\n",
        "    all_path=[]\n",
        "    with VideoFileClip(video_path) as video:\n",
        "        duration = int(video.duration)\n",
        "        for i in range(0, duration, interval):\n",
        "            frame = video.get_frame(i)\n",
        "            frame_image = Image.fromarray(frame)\n",
        "            frame_name = format_frame_name(i)\n",
        "            frame_path = os.path.join(output_folder, frame_name)\n",
        "            # print(f'what is frame path {frame_path}')\n",
        "            frame_image.save(frame_path)\n",
        "            # print(f\"Captured and saved {frame_name}.\")\n",
        "            all_path.append(frame_path)\n",
        "    print(\"All frames have been captured and saved.\")\n",
        "    return all_path"
      ],
      "metadata": {
        "id": "6NfBvU9399yz"
      },
      "id": "6NfBvU9399yz",
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "def calculate_image_dimensions(image_path, max_width, max_height):\n",
        "    # Initialize variables to store the new width and height\n",
        "    new_width = None\n",
        "    new_height = None\n",
        "\n",
        "    # Check the first image to calculate the new dimensions\n",
        "    first_image = Image.open(image_path)\n",
        "    first_image_width, first_image_height = first_image.size\n",
        "\n",
        "    # Calculate the scaling factor based on the first image\n",
        "    width_ratio = max_width / first_image_width\n",
        "    height_ratio = max_height / first_image_height\n",
        "    scale_factor = min(width_ratio, height_ratio)\n",
        "\n",
        "    # Calculate the new width and height based on the first image\n",
        "    new_width = first_image_width * scale_factor\n",
        "    new_height = first_image_height * scale_factor\n",
        "\n",
        "    return new_width, new_height\n",
        "\n",
        "def add_images_to_word_document(df, word_fname, max_width, max_height, num_images=10):\n",
        "    print('Now I am saving the all the transcription and images in word')\n",
        "\n",
        "\n",
        "        # Check if word_fname already exists\n",
        "    if os.path.isfile(word_fname):\n",
        "        # If it exists, create a new name with an addition (e.g., v1, v2, v3)\n",
        "        base_name, extension = os.path.splitext(word_fname)\n",
        "        counter = 1\n",
        "        while os.path.isfile(word_fname):\n",
        "            word_fname = f'{base_name}_v{counter}{extension}'\n",
        "            counter += 1\n",
        "\n",
        "\n",
        "            # Check the first image in the loop to calculate the new dimensions\n",
        "    first_image_path = df.iloc[0]['image_path']\n",
        "\n",
        "\n",
        "    new_width, new_height = calculate_image_dimensions(first_image_path, max_width, max_height)\n",
        "    doc = Document()\n",
        "    for index, row in df.iterrows():\n",
        "    # for index, row in df.head(num_images).iterrows():\n",
        "        # Add the image\n",
        "        image_path = row['image_path']\n",
        "\n",
        "        doc.add_picture(image_path, width=Inches(new_width), height=Inches(new_height))\n",
        "        # transcription_text = row['Transcription']['text']\n",
        "\n",
        "        doc.add_paragraph(row['Transcription']['text'])\n",
        "\n",
        "        # Add a new line between transcriptions\n",
        "        doc.add_paragraph(\"\\n\")\n",
        "\n",
        "    # Save the document\n",
        "    doc.save(word_fname)\n",
        "    print('Complete saving the all the transcription and images in word')\n",
        "    print(f'The file is saved as  {word_fname}')"
      ],
      "metadata": {
        "id": "9yiXvfirStvG"
      },
      "id": "9yiXvfirStvG",
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def process_video(durat, youtube_video_url):\n",
        "    # Rest of your existing code\n",
        "\n",
        "    video_url = youtube_video_url\n",
        "    video_path, video_title = download_youtube_video(video_url)\n",
        "    output_folder = create_valid_folder_name(video_title)\n",
        "\n",
        "    # downloaded_audio_path = download_audio_from_youtube(youtube_video_url)\n",
        "    downloaded_audio_path=video_path\n",
        "    segments = segment_audio(downloaded_audio_path, segment_duration_seconds=durat)\n",
        "\n",
        "\n",
        "\n",
        "    all_path = capture_frames(video_path, interval=durat, output_folder=output_folder)\n",
        "\n",
        "    df2 = pd.DataFrame(all_path, columns=['image_path'])\n",
        "\n",
        "    df = pd.DataFrame(segments, columns=['SegmentPath'])\n",
        "    df['SegmentNumber'] = df['SegmentPath'].apply(lambda x: int(x.split('_')[-1].split('.')[0]))\n",
        "    df = df.sort_values(by='SegmentNumber')\n",
        "\n",
        "    print(' This may take some time, but i am current whispering shhhhhh')\n",
        "    df['Transcription'] = df['SegmentPath'].apply(lambda x: model.transcribe(x))\n",
        "    combined_df = pd.concat([df2, df], axis=1)\n",
        "    # Set the maximum width and height for the image (in inches)\n",
        "    max_width = 6.5  # 8.5 inches - 1 inch margin on each side\n",
        "    max_height = 9.0  # 11 inches - 1 inch margin on top and bottom\n",
        "    output_word_file=f'{video_title[:12]}.docx'\n",
        "    add_images_to_word_document(combined_df, output_word_file, max_width, max_height, num_images=10)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "S538tLWlkbLO"
      },
      "id": "S538tLWlkbLO",
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "youtube_url=\"https://youtu.be/zE5615wKmq0?list=PLyqSpQzTE6M88PUx4AtV1WWNeKYVIvPAl\"\n",
        "duration=20\n",
        "\n",
        "process_video(duration, youtube_url)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "230hU8U-Vjjr",
        "outputId": "99d4cced-c84f-4549-f5af-e887eda28d54"
      },
      "id": "230hU8U-Vjjr",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading YouTube video...\n",
            "Video Lecture 23 - Concept Development has been downloadeded successfully.\n",
            "Capturing frames from the video into folder 'Lecture_23__Concept_Development'...\n",
            "All frames have been captured and saved.\n",
            " This may take some time, but i am current whispering shhhhhh\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}